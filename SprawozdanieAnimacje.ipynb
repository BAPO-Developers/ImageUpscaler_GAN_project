{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26f64fbc",
   "metadata": {},
   "source": [
    "<h1><center>UCZENIE MASZYNOWE W ANIMACJACH</center></h1>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<center><img src=\"pictures/logo.png\"/></center>\n",
    "<br></br>\n",
    "\n",
    "<h2><center>Prowadzący: mgr inż. Szymon Datko</center></h2>\n",
    "\n",
    "<h3>Grupa Alfa</h3>\n",
    "<ul>\n",
    "<li>Adam Polerowicz 243442</li>\n",
    "<li>Marcin Wawszczak</li>\n",
    "<li>Bartosz Olszewski</li>\n",
    "</ul>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<h2>Temat: Wykorzystanie sieci GAN do zadania upscalingu</h2>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<h4>Zadanie polegało na wykorzystaniu sieci GAN do zwiększania rozdzielczości obrazów.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41202da1",
   "metadata": {},
   "source": [
    "#  Implementacje open-source\n",
    "\n",
    "<br></br>\n",
    "\n",
    "Sieci GAN są stosunkowo nową architekturą. Szybko jednak znalazły zastosowanie przy zadaniach upscalingu i augmentacji. Na początku należało przeprowadzić analize dostępnych rozwiązań. Powszechnie znanym repozytorium wszelkich open-sourcowych projektów jest Github. Pośród wielu implementacji sieci GAN do zwiększania rozdzielczości obrazów [1][2], na szczególną uwagę zasługuje repozytorium IBM [3]. Zawiera ono miedzy innymi kod do szybkiej implementacji upscalera. Jest on w stanie zwiększyć rozdzielczość obrazu czterokrotnie. Użyty model został wytrenowany na 600 000 obrazach z OpenImagesV4 [4]. Jakość uzyskiwanych wyników została sprawdzona na trzech zbiorach danych (Set4, Set14 oraz BSD100). Użyto metryk PSNR (ang. peak signal to noise ratio) oraz SSIM (structural similarity index). Repozytorium zawiera API, które ułatwia pracę z modelem. Jego uruchomienie sprowadza się do ściągnięcia obrazu z rejestru Quay.io [5] poprzec Dockera [6]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d3848",
   "metadata": {},
   "source": [
    "Przeprowadzono kilkanaście inferencji z modelem używając obrazów znalezionych w grafice Google. Jakość uzyskanych obrazów oceniona organoleptycznie została określona jako nieidealna i w zależności od użytego obrazu pojawiały się artefakty.\n",
    "\n",
    "<br></br>\n",
    "<h4> Przykłady obrazów mocno odbiegających od oryginału </h4>\n",
    "<br></br>\n",
    "\n",
    "![10](.\\pictures\\Enchanced\\10en.png)\n",
    "![3](.\\pictures\\Enchanced\\3en.png)\n",
    "\n",
    "<br></br>\n",
    "<h4> Przykłady obrazów prawie idealnych (drobne artefakty) </h4>\n",
    "<br></br>\n",
    "\n",
    "![6](.\\pictures\\Enchanced\\6en.png)\n",
    "![4](.\\pictures\\Enchanced\\4en.png)\n",
    "\n",
    "<br></br>\n",
    "<h4> Przykłady obrazów nie do odróżnienia z oryginalnymi </h4>\n",
    "<br></br>\n",
    "\n",
    "![5](.\\pictures\\Enchanced\\5en.png)\n",
    "![7](.\\pictures\\Enchanced\\7en.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45821d86",
   "metadata": {},
   "source": [
    "# Metryki\n",
    "<br></br>\n",
    "W celu przeprowadzenia porównania obrazów referencyjnych i po uscalingu na podstawie innej niż organoleptyczna stworzono skrypty do liczenia SSIM oraz MSE (ang. mean square error). Nie można jednak całkowicie wykluczyć bliżej nieokreślonej metryki jaką jest ocena organoleptyczna. Autorzy pracy naukowej [7], na podstawie której został stworzony model IBM sugerują, że MOS (z ang. Mean Opinion Score) jest najlepszą metryką do oceny modelu tego typu. MOS jest średnią opinią tworzoną przez subiektywną opinię ludzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dec521fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "def resize_images(path : str, extension='png'):\n",
    "    resize_factor = 4\n",
    "    base_path = Path(path)\n",
    "\n",
    "    for image_path in base_path.glob('*.' + extension):\n",
    "        img = Image.open(image_path)\n",
    "        size = img.size\n",
    "        new_size = (int(size[0] / resize_factor), int(size[1] / resize_factor))\n",
    "        print('Original: {}'.format(size) + ' New: {} \\t\\t File {}'.format(new_size, image_path.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f51cfb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (1920, 1080) New: (480, 270) \t\t File 1.png\n",
      "Original: (1920, 1080) New: (480, 270) \t\t File 10.png\n",
      "Original: (1920, 1080) New: (480, 270) \t\t File 2.png\n",
      "Original: (1920, 1080) New: (480, 270) \t\t File 3.png\n",
      "Original: (1920, 1080) New: (480, 270) \t\t File 4.png\n",
      "Original: (1920, 1080) New: (480, 270) \t\t File 5.png\n",
      "Original: (1920, 1080) New: (480, 270) \t\t File 6.png\n",
      "Original: (1920, 1080) New: (480, 270) \t\t File 7.png\n",
      "Original: (1920, 1080) New: (480, 270) \t\t File 8.png\n",
      "Original: (1920, 1080) New: (480, 270) \t\t File 9.png\n",
      "Original: (1920, 1080) New: (480, 270) \t\t File 9_white.png\n"
     ]
    }
   ],
   "source": [
    "resize_images('./pictures/Originals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3817cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "from pandas import *\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def mse(imageA, imageB):\n",
    "\terr = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "\terr /= float(imageA.shape[0] * imageA.shape[1])\n",
    "\treturn err\n",
    "\n",
    "def print_compare(pathA : str, pathB : str, suffix='', extensions=['png', 'png']):\n",
    "    base_path = Path(pathA).parent\n",
    "    names, mses, sims = [], [], [] \n",
    "    \n",
    "    for org_path in Path(pathA).glob('*.' + extensions[0]):\n",
    "        orig = cv2.imread(str(Path(pathA).joinpath(str(org_path.stem) + '.' + extensions[0])))\n",
    "        ench = cv2.imread(str(Path(pathB).joinpath(str(org_path.stem) + suffix + '.' +  extensions[1])))\n",
    "        orig = cv2.cvtColor(orig, cv2.COLOR_BGR2GRAY)\n",
    "        ench = cv2.cvtColor(ench, cv2.COLOR_BGR2GRAY)\t\n",
    "        mses.append(float(\"{:.2f}\".format(mse(orig, ench))))\n",
    "        sims.append(float(\"{:.4f}\".format(ssim(orig, ench))))\n",
    "        names.append(org_path.stem)\n",
    "\n",
    "    merge = [names, mses, sims, [element * 100 for element in sims]]\n",
    "    df = DataFrame(merge)\n",
    "    col = ['Image name', 'MSE', 'SSIM', 'SSIM %']\n",
    "    df.insert(0, \"Id\", col, True)\n",
    "    print(df.to_string(index=False))\n",
    "    print('Mean MSE: {:.2f}'.format(np.mean(mses)))\n",
    "    print('Mean SSIM: {:.2f} %'.format(np.mean(merge[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ed4fdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id       0       1         2       3       4      5       6       7        8         9      10\n",
      "Image name       1      10         2       3       4      5       6       7        8         9 9_white\n",
      "       MSE  835.81  393.05  24262.87  170.67   19.74  80.59   32.44    6.84  3119.12  11698.99  206.67\n",
      "      SSIM  0.4739  0.8666    0.5329  0.7372  0.9307  0.903  0.9445  0.9838   0.8559    0.2107  0.7717\n",
      "    SSIM %   47.39   86.66     53.29   73.72   93.07   90.3   94.45   98.38    85.59     21.07   77.17\n",
      "Mean MSE: 3711.53\n",
      "Mean SSIM: 74.64 %\n"
     ]
    }
   ],
   "source": [
    "print_compare('./pictures/Originals/', './pictures/Enchanced/', 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7894da7b",
   "metadata": {},
   "source": [
    "# Samodzielne implementacje\n",
    "\n",
    "<br></br>\n",
    "\n",
    "Po wykonaniu przeglądu dostępnych rozwiązań podjęto się zadania wytrenowania sieci GAN samodzielnie. Wykorzystano w tym celu PyTorch-ową [8] implementacje architektury SRGAN [9]. Sieci GAN ze względu na swoją strukturę można opisać jako dwie współpracujące sieci. Oznacza to, że wytrenowanie sieci na podobnej ilości obrazów jak referencyjna sieć (600 000) wymagałoby dużo zasobów zarówno sprzętowych jak i czasowych. Postanowiono przeprowadzić porównanie sieci IBM z siecią  wytrenowaną na dużo mniejszym zbiorze ograniczonym do konkretnej tematyki. Wykorzystano zbiór danych URBAN100, który zawiera 100 zdjęć architektury i budowli z całego świata. Aby móc wytrenować sieć na tych danych należało skorzystać ze skryptu dzielącego zbiór na zbiór treningowy i walidacyjny. Strategia tworzenia danych potrzebnych do wytrenowania sieci sprowadza się na podziale pojedyńczego zdjęcia na kilkanaście równych sobie co do wymiarów obrazów. W ten sposób ze stu obrazów postał zbiór treningowy, który liczy 29 020 instancji oraz walidacyjny z 1 451 instancjami. Sieć została wytrenowana na sprzęcie o następujących parametrach:\n",
    "<ul>\n",
    "    <li> CPU i7 6500u </li>\n",
    "    <li> GPU GTX 940mx </li>\n",
    "</ul>\n",
    "Trening trwał około 36 godzin i został zatrzymany przy 33 epoce (early stopping).\n",
    "\n",
    "Po uzyskaniu najlepszego modelu przystąpiono do porównania z siecią IBM. Użyto dzisięciu obrazów, które przedstawiały budowle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7c2884f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (480, 360) New: (120, 90) \t\t File 1.jpg\n",
      "Original: (384, 384) New: (96, 96) \t\t File 2.jpg\n",
      "Original: (264, 400) New: (66, 100) \t\t File 3.jpg\n",
      "Original: (336, 224) New: (84, 56) \t\t File 4.jpg\n",
      "Original: (1024, 892) New: (256, 223) \t\t File 5.jpg\n",
      "Original: (400, 348) New: (100, 87) \t\t File 6.jpg\n",
      "Original: (300, 300) New: (75, 75) \t\t File 7.jpg\n",
      "Original: (300, 300) New: (75, 75) \t\t File 8.jpg\n",
      "Original: (316, 224) New: (79, 56) \t\t File 9.jpg\n"
     ]
    }
   ],
   "source": [
    "resize_images('./pictures/Original', 'jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cc0091",
   "metadata": {},
   "source": [
    "W ten sposób przygotowane obrazy podano sieci IBM i SRGAN jako dane wejściowe. W efekcie otrzymano czterokrotnie większe obrazy. Sieć SRGAN domyślnie zapisuje kanały obrazów w kolejności BGR a nie RGB. W celu ujednolicenia kolejności kolorów przerobiono je na RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56a0d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def bgr2rgb(path : str, extension='png'):\n",
    "    test_path = Path(path)\n",
    "\n",
    "    for path in test_path.glob('*.' + extension):\n",
    "        img = cv2.imread(str(path))\n",
    "        img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c8e3c",
   "metadata": {},
   "source": [
    "Nastepnie porównano wyniki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5e605b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id       0       1      2       3       4       5       6        7       8\n",
      "Image name       1       2      3       4       5       6       7        8       9\n",
      "       MSE  250.75  224.36  352.0  392.52  488.94  517.36   652.6  1523.54  567.86\n",
      "      SSIM  0.6504  0.6873  0.722  0.6343  0.6703  0.7356  0.6299   0.4812  0.7047\n",
      "    SSIM %   65.04   68.73   72.2   63.43   67.03   73.56   62.99    48.12   70.47\n",
      "Mean MSE: 552.21\n",
      "Mean SSIM: 65.73 %\n"
     ]
    }
   ],
   "source": [
    "print_compare('./pictures/IBM','./pictures/Original', extensions=['png', 'jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31f414c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id       0       1       2       3       4       5       6        7       8\n",
      "Image name       1       2       3       4       5       6       7        8       9\n",
      "       MSE  251.84  203.31   234.7  355.36  395.97   459.6  449.35  1600.13  475.05\n",
      "      SSIM  0.6688  0.6945  0.7994  0.6518  0.7014  0.7565  0.7033    0.495  0.7259\n",
      "    SSIM %   66.88   69.45   79.94   65.18   70.14   75.65   70.33     49.5   72.59\n",
      "Mean MSE: 491.70\n",
      "Mean SSIM: 68.85 %\n"
     ]
    }
   ],
   "source": [
    "print_compare('./pictures/SRGAN','./pictures/Original', extensions=['jpg', 'jpg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c91d084",
   "metadata": {},
   "source": [
    "# Porównanie IBM i SRGAN do oryginalnych zdjęć \n",
    "<br></br>\n",
    "Analizując wyniki porównania powiększonych zdjęć do oryginalnych stwierdzono kilkuprocentową przewagę sieci SRGAN. Różnica w metryce MSE wynosi ponad 12%. Oznacza to, że sieć wyszkolona w wąskiej tematyce poradziła sobie lepiej z upscalingiem obrazów nawiązujących do niej. Metryka MOS (użyto trzech osób) potwierdziła subtelną przewagę sieci SRGAN.\n",
    "<br></br>\n",
    "Kolejność:\n",
    "<ol>\n",
    "    <li>Oryginał</li>\n",
    "    <li>SRGAN</li>\n",
    "    <li>GAN</li>\n",
    "</ol>\n",
    "\n",
    "<br></br>\n",
    "![3](.\\pictures\\Original\\3.jpg)\n",
    "![3](.\\pictures\\SRGAN\\3.jpg)\n",
    "![3](.\\pictures\\IBM\\3.png)\n",
    "<br></br>\n",
    "![9](.\\pictures\\Original\\9.jpg)\n",
    "![9](.\\pictures\\SRGAN\\9.jpg)\n",
    "![9](.\\pictures\\IBM\\9.png)\n",
    "<br></br>\n",
    "![5](.\\pictures\\Original\\5.jpg)\n",
    "![5](.\\pictures\\SRGAN\\5.jpg)\n",
    "![5](.\\pictures\\IBM\\5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee663c6",
   "metadata": {},
   "source": [
    "# Zmiana czynnika upscalingu\n",
    "\n",
    "<br></br>\n",
    "\n",
    "Sieć SRGAN opisywana w powyższych rozdziałach została wytrenowana do upscalingu obrazów o czynnik równy 4. Pozwoliło to na porównanie wytrenowanego modelu z modelem IBM. Podjęto póbę stworzenia sieci, która cechowałaby się czynnikiem upscalingu równym 8 lub 16. \n",
    "Skorzystano z gotowej implementacji sieci [10]. W celu wytrenowania modelu użyto zbioru DIV2K [11]. Model wykorzystuje bloki pola receptywnego (RFB), które umożliwiają wydobycie infromacji z obrazów w większej skali i zwiększenie możliwości dyskryminacyjnych modelu. Ponadto, zamiast używać dużych jąder splotowych w wieloskalowym bloku receptywnym, model korzysta z kilku małych jąder, dzięki czemu jest w stanie wyodrębnić większe szczegóły z obrazów i zmniejszyć złożoność obliczeń. Ostatnią włąsnością modelu jest fakt, że naprzemiennie wykorzystuje różne metody upsamplingu, aby zmniejszyć dużą złożoność obliczeń i zachować zadowalającą wydajność.\n",
    "Przygotowano zbiór danych, dzieląc go na zbiór treningowy i walidacyjny. Z powodzeniem ropoczęto proces trenowania sieci, który początkowo przewidziany na 100 epok, w ciągu 36 godzin nieustannej pracy zakończono (umyślnie) w połowie pierwszej epoki. Złożoność modelu wraz z faktem dużo większych danych uczących (rozmiar zdjęcia) oraz małej mocy obliczeniowej okazała się być powodem niepowodzenia wytrenowania sieci. \n",
    "\n",
    "Podjęto próbę wytrenowania innego modelu z czynnikiem upscalingu równym 2. Wykorzystano kod z repozytorium SRGAN [12]. Do treningu ponownie użyto zbioru DIV2K, który przygotowano dzieląc każdy z obrazów na kilka o mniejszej rozdzielczości (z pojedynczego 2560x1440 na wiele 600x600 pikseli). Z powodzeniem uruchomiono trening i z takim samym rezultatem go zakończono. Poniżej przedstawiono uzyskane rezultaty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a3b9b5",
   "metadata": {},
   "source": [
    "# Kod\n",
    "\n",
    "<br></br>\n",
    "https://github.com/BAPO-Developers/ImageUpscaler_GAN_project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c620bca",
   "metadata": {},
   "source": [
    "# Referencje\n",
    "<br></br>\n",
    "<ul>\n",
    "    <li>[1] https://github.com/brade31919/SRGAN-tensorflow</li>\n",
    "    <li>[2] https://github.com/trevor-m/tensorflow-SRGAN</li>\n",
    "    <li>[3] https://github.com/IBM/MAX-Image-Resolution-Enhancer</li>\n",
    "    <li>[4] https://storage.googleapis.com/openimages/web/index.html</li>\n",
    "    <li>[5] https://quay.io </li>\n",
    "    <li>[6] https://www.docker.com</li>\n",
    "    <li>[7] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, W. Shi, Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, ArXiv, 2017.</li>\n",
    "    <li>[8] https://pytorch.org </li>\n",
    "    <li>[9] https://github.com/Lornatang/SRGAN-PyTorch </li>\n",
    "    <li>[10] https://github.com/Lornatang/RFB_ESRGAN-PyTorch </li>\n",
    "    <li>[11] https://data.vision.ee.ethz.ch/cvl/DIV2K/ </li>    \n",
    "    <li>[12] https://github.com/leftthomas/SRGAN </li>    \n",
    "    \n",
    "   \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c6b1af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
